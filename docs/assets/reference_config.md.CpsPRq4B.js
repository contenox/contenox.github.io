import{_ as i,o as a,c as s,ag as t}from"./chunks/framework.DQQHyi2x.js";const k=JSON.parse('{"title":"Configuration","description":"","frontmatter":{},"headers":[],"relativePath":"reference/config.md","filePath":"reference/config.md"}'),n={name:"reference/config.md"};function o(l,e,r,d,p,h){return a(),s("div",null,[...e[0]||(e[0]=[t(`<h1 id="configuration" tabindex="-1">Configuration <a class="header-anchor" href="#configuration" aria-label="Permalink to &quot;Configuration&quot;">​</a></h1><p>The Contenox runtime is configured via <code>.contenox/config.yaml</code>.</p><p>When you run <code>vibe init</code>, a default configuration is generated. <code>vibe</code> looks for this directory in your current working directory, then walks up to the root, and finally checks your home directory (<code>~/.contenox/</code>).</p><h2 id="default-config-yaml" tabindex="-1">Default <code>config.yaml</code> <a class="header-anchor" href="#default-config-yaml" aria-label="Permalink to &quot;Default \`config.yaml\`&quot;">​</a></h2><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">version</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">default_provider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">ollama</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">default_model</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">qwen2.5:7b</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">providers</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  ollama</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    base_url</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">http://localhost:11434</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">  openai</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">    api_key</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;</span></span></code></pre></div><h2 id="settings" tabindex="-1">Settings <a class="header-anchor" href="#settings" aria-label="Permalink to &quot;Settings&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Key</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td><code>default_provider</code></td><td>The fallback provider if a chain doesn&#39;t specify one</td><td><code>ollama</code></td></tr><tr><td><code>default_model</code></td><td>The fallback model if a chain doesn&#39;t specify one</td><td><code>gpt-4o</code></td></tr><tr><td><code>providers.&lt;name&gt;</code></td><td>Provider-specific connection settings</td><td>see below</td></tr></tbody></table><h2 id="supported-providers" tabindex="-1">Supported Providers <a class="header-anchor" href="#supported-providers" aria-label="Permalink to &quot;Supported Providers&quot;">​</a></h2><p>Contenox uses a unified translation layer, meaning you can swap providers per-task in your chains without changing the prompt format or tool schemas.</p><ol><li><strong><code>ollama</code></strong>: Requires <code>base_url</code> (usually <code>http://localhost:11434</code>).</li><li><strong><code>openai</code></strong>: Requires <code>api_key</code> (or uses <code>OPENAI_API_KEY</code> from environment).</li><li><strong><code>vllm</code></strong>: Exposes an OpenAI-compatible endpoint. Requires <code>base_url</code>.</li><li><strong><code>gemini</code></strong>: Requires <code>api_key</code>.</li></ol>`,10)])])}const g=i(n,[["render",o]]);export{k as __pageData,g as default};
